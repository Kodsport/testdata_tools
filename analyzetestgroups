#!/usr/bin/env python3

# Assumptions:
# Correctness:
#  Secret groups are numbered data/secret/group1, data/secret/group2, ...
# Typographical (otherwise ugly output):
#  Times are <= 9.99s 
#  At most 9 groups 
#  Points are at most three digits

import re, sys, subprocess, yaml, argparse, io
from itertools import groupby
from pathlib import Path
from collections import defaultdict
from typing import List, Optional, Tuple

def parse_args():
    # Returns full path to problem directory and output of verifyproblem 
    argsparser = argparse.ArgumentParser(
            description = r"""
            Summarise verifyproblem's log of a scoring problem built with testdata_tools.
            If submission source contains 
            '@EXPECTED_GRADES@ WA WA WA WA AC WA'
            somwhere, e.g., as a comment, also compare the outcome of secret test
            groups with the expected outcome.
            """)
    argsparser.add_argument('problemdir', help='Path to problem directory')
    argsparser.add_argument('-f', '--file', dest = 'logfile',
                            type=open, help='read logfile instead of running verifyproblem -l info')
    args = argsparser.parse_args()

    assert Path(args.problemdir).is_dir()
    problempath = Path(args.problemdir).resolve()

    if not args.logfile:
        verifyproblem = subprocess.Popen(['verifyproblem', args.problemdir, '-l info'], 
                                         stdout=subprocess.PIPE, 
                                         encoding = 'utf-8', 
                                         universal_newlines = True, 
                                         bufsize = 1)
        print ('Running', ' '.join(verifyproblem.args), '...', end ='\r')
        inputstream = verifyproblem.stdout
    else:
        inputstream = args.logfile 
    return problempath, inputstream

patterns = { 
    'END_SUBMISSION': 
        re.compile(
        r"""
        (?P<type>\S+)
        \s+
        submission
        \s+
        (?P<filename>\S+)
        \s+
        \((?P<language>[^)]+)\)
        \s+
        (?P<status>\S+)
        \s+
        (?P<grade>\S+)
        \s+
        (\((?P<points>\d+)\)\s+)?
        \[.*CPU:\s(?P<maxtime>\d+.\d+)s.*\]
        """,
        re.VERBOSE),
    'FIRST_LINE': 
        re.compile(r"Loading problem (?P<problemname>\S+)"),
    'TESTGROUP_GRADE': 
        re.compile(
        r"""INFO\ :\ Grade\ on\ test\ case\ group\ data/
        (?P<type>sample|secret/group)
        ((?P<number>\d+))?
        \s+
        is
        \s+
        (?P<grade>\S+)""",
        re.VERBOSE),
    'START_SUBMISSION':
        re.compile( r"INFO : Check (?P<type>\S+) submission (?P<filename>\S+)"),
    'START_TESTGROUP': 
        re.compile( r"INFO : Running on test case group data/(sample|secret/group(?P<number>\d+))"),
    'AC_TC_RESULT': 
        re.compile(
        r"""[T|t]est\ file\ result.*AC.*CPU:\s
        (?P<time>\d+.\d+)
        .*
        test\ case\ (sample|secret/group\d)/
        (?P<case>[^\]]+)
        """, re.VERBOSE),
    'TIMELIMIT': 
        re.compile(r"setting timelim to (?P<limit>\d+) secs, safety margin to (?P<safety>\d+) secs")
    }

expected_score_pattern = re.compile(r"@EXPECTED_GRADES@ (?P<grades>((WA|AC|TLE|RTE|MLE)\s*)+)")

class Verdict:
    def __init__(self, grade, time = None):
        self.grade = grade
        if grade == 'AC': 
            self.time = time # time can be None for AC sample group
        else:
            self.time = None

    def __str__(self):
        res = '\033[32m' if self.grade == 'AC' else '\033[91m'
        res += f'{self.grade}\033[0m'
        if self.time is not None:
            res += f':{self.time}s'
        return res
    
    def __format__(self, f):
        return str(self).__format__(f)

class Submission:
    subdir = { 
            'AC':  'accepted',
            'PAC': 'partially_accepted',
            'WA':  'wrong_answer',
            'TLE': 'time_limit_exceeded' }

    def __init__(self, expected_total_grade, filename):
        self.filename = filename 
        self.expected_total_grade = expected_total_grade
        self.verdicts: List[Verdict] = []
        self.maxtime: Optional[float] = None
        self.points: Optinal[int] = None
        self.fullpath = Path(Submission.subdir[expected_total_grade]) / filename

    def __str__(self):
        return self.filename

def parse_log(problempath, inputstream) -> Tuple[Tuple[int, int], List[Submission]]:
    # Analyse output generated by verifyproblem <problemname> -l info
    timelimit = None
    timelimit_safe = None
    submissions: List[Submission] = []

    s = tc_id = None

    for line in inputstream:
        for p in patterns:
            match = patterns[p].search(line)
            if match:
                break
        else:
            continue
        d = match.groupdict()
        if p  == 'FIRST_LINE':
            problemname = d['problemname']
            if problemname != problempath.stem:
                exit(f'FATAL: Problem directory does not match log file ({problemname}). Aborting...')
            with open(problempath / 'problem.yaml', encoding = 'utf-8') as file:
                problemtype = yaml.safe_load(file).get('type')
                if problemtype != 'scoring':
                    exit(f'FATAL: {problemname} is is not a scoring problem. Aborting...')
            print (' '*80, end = '\r')
            print(f'\033[01mScoring problem: {problemname}\033[0m')
        elif p == 'START_SUBMISSION':
            s = Submission(d['type'], d['filename'])
        elif p == 'START_TESTGROUP':
            tc_times: List[float] = []
        elif p == 'AC_TC_RESULT':
            print (problemname, s, end ='\r')
            tc_times.append(float(d['time']))
            tc_id = d['case']
        elif p == 'TESTGROUP_GRADE':
            grade = d['grade']
            if d['type'] == 'sample': 
                assert s is not None and len(s.verdicts) == 0
            else:  
                assert s is not None and len(s.verdicts) == int(d['number'])
                assert grade != 'AC' or len(tc_times) # secret test group must have at least one tc for AC
            s.verdicts.append(Verdict(grade, max(tc_times) if len(tc_times) else None))
        elif p == 'END_SUBMISSION':
            assert s is not None
            s.points = int(d["points"] or "0")
            s.maxtime = float(d["maxtime"])
            submissions.append(s)
        elif p == 'TIMELIMIT':
            timelimits = int(d['limit']), int(d['safety'])
        statusline = f'Submission {s}, test case {tc_id}'
        print (' '*80, end = '\r')
        print (statusline[:80], end = '\r')

    assert timelimits is not None 
    return timelimits, submissions

def pretty_yn(value: bool) -> str:
    # green y or red n
     return '\033[32my\033[0m' if value else '\033[91mn\033[0m'

def print_table(submissions):
    alignto = max (len(str(sub)) for sub in submissions)
    if alignto < len('Submission'):
        alignto = len('Submission')
    print("\033[01m", end = '')
    print(f"{'Submission':{alignto}} Sample  ", end = ' ')
    print(' '.join(f'Group {i} ' for i in range(1, num_secret_groups + 1)), end = ' ')
    print('Pts Time  Expected\033[0m')
    for s in submissions:
        print (f"{s.filename:{alignto}}", end = ' ')
        for verdict in s.verdicts:
            print (f'{verdict:17}', end = ' ')
        print(f"{s.points:3}", end = ' ')
        print(f"{s.maxtime:4.2f}s", end = ' ')

        # get the expected grades from the file (if exist), default to AC for AC submissions
        expected_grades = None
        path = problempath / 'submissions' / s.fullpath
        with open(path, encoding = 'utf-8') as sourcefile:
            for line in sourcefile:
                m = expected_score_pattern.search(line)
                if m:
                    expected_grades = m.group("grades").split()
                    assert len(expected_grades) == num_secret_groups
                    break
        if not expected_grades and s.expected_total_grade == 'AC':
            expected_grades = ['AC'] * num_secret_groups

        if expected_grades:
            summary = [pretty_yn(expected_grades[i] == s.verdicts[i+1].grade) for i in range(num_secret_groups)]
        else:
            summary = ['.'] * num_secret_groups

        print (''.join(summary))

def check_distinguished(submissions):
    accepting_subs = defaultdict(list)
    for s in submissions:
        for group, v in enumerate(s.verdicts):
            if v.grade == 'AC':
                accepting_subs[group].append(s)
    ok = True
    for i in range(1, num_secret_groups):
        for j in range(i + 1, num_secret_groups + 1):
            if accepting_subs[i] == accepting_subs[j]:
                print (f'\033[91mWarning: \033[0mNo submission distinguishes test groups {i} and {j}')
                ok = False
    if ok:
        print ('\033[32mOK: \033[0mAll secret test groups distinguished by some submission')

problempath, inputstream = parse_args()
timelimits, submissions = parse_log(problempath, inputstream)
num_secret_groups = max(len(s.verdicts) for s in submissions) - 1 
assert all (len(s.verdicts) == 1 + num_secret_groups for s in submissions)

submissions.sort(key=lambda d: (-d.points, d.maxtime))

print_table(submissions)
print(f"Time limit: {timelimits[0]}s, safe: {timelimits[1]}s")
check_distinguished(submissions)
